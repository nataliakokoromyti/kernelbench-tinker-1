#!/usr/bin/env python3
"""
CLI entrypoint for evaluating trained KernelBench models.

Usage:
    python -m kernelbench_tinker.scripts.eval_kernel_rl \
        checkpoint_path=./runs/kernelbench_tinker_v1/checkpoints/final \
        level=1 \
        output_path=./runs/kernelbench_tinker_v1/eval_results.json

Or with the installed script:
    eval-kernel-rl checkpoint_path=... level=...
"""

from __future__ import annotations

import asyncio
import json
import logging
import os
from dataclasses import dataclass, asdict
from typing import Any

import chz
import tinker
from tqdm import tqdm

from tinker_cookbook import renderers, tokenizer_utils
from tinker_cookbook.completers import TinkerTokenCompleter

from kernelbench_tinker.env import setup_environment
from kernelbench_tinker.envs.kernelbench_client import (
    KernelBenchProblem,
    evaluate_kernel_async,
    get_problem_ids,
    parse_kevin_response,
    parse_structured_response,
)
from kernelbench_tinker.training.loop import KernelBenchTokenCompleter
from kernelbench_tinker.training.models import get_renderer_name_for_model

logger = logging.getLogger(__name__)


@chz.chz
class EvalConfig:
    """Configuration for model evaluation."""

    # Model/checkpoint configuration
    checkpoint_path: str = ""  # Path to checkpoint or "tinker://..." path
    model_name: str = "Qwen/Qwen2.5-Coder-7B-Instruct"  # For tokenizer/renderer

    # Evaluation configuration
    level: int = 1
    start_problem: int | None = None
    end_problem: int | None = None
    backend: str = "triton"
    dataset_src: str = "huggingface"

    # Generation configuration
    max_tokens: int = 4096
    temperature: float = 0.0  # Greedy for eval
    top_p: float = 1.0  # Nucleus sampling (1.0 = disabled)
    seed: int | None = None  # Random seed for generation (null = random)
    num_samples: int = 1  # Samples per problem

    # Evaluation settings
    num_correct_trials: int = 5
    measure_performance: bool = True
    num_perf_trials: int = 100
    timing_method: str = "cuda_event"
    precision: str = "fp32"
    check_for_excessive_speedup: bool = True
    excessive_speedup_threshold: float = 10.0

    # Modal configuration
    modal_gpu_type: str = "A100"
    modal_timeout: float = 120.0

    # Prompt configuration
    prompt_option: str = "one_shot"
    prompt_include_hardware: bool = False
    prompt_gpu_name: str | None = None

    # Output
    output_path: str = "./eval_results.json"
    max_kernel_code_chars: int | None = 500  # Set to None to store full code

    # TensorBoard logging (optional, to log eval metrics alongside training)
    tensorboard_log_dir: str | None = None  # If provided, log eval metrics to TensorBoard
    tensorboard_step: int = 0  # Step to log eval metrics at

    # Multi-turn inference
    multiturn_enabled: bool = False
    multiturn_max_turns: int = 8
    kevin_prompt: bool = False  # Use Kevin's exact prompt format
    inject_think_token: bool = False  # Append <think>\n to generation prompts
    prompt_max_tokens: int | None = None  # Token budget for history truncation

    # Tinker API
    base_url: str | None = None


@dataclass
class EvalResult:
    """Result for a single problem."""
    level: int
    problem_id: int
    samples: list[dict[str, Any]]
    best_correct: bool
    best_compiled: bool
    best_speedup: float | None


def _speedup_value(sample: dict[str, Any]) -> float:
    speedup = sample.get("speedup")
    return float(speedup) if isinstance(speedup, (int, float)) else 0.0


async def generate_kernel(
    sampling_client: tinker.SamplingClient,
    problem: KernelBenchProblem,
    renderer: renderers.Renderer,
    max_tokens: int,
    temperature: float,
) -> str:
    """Generate a kernel for a problem (single-turn)."""
    from kernelbench_tinker.envs.kernelbench_env import build_system_prompt

    system_prompt = build_system_prompt(problem.backend)
    messages: list[dict[str, str]] = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": problem.prompt})

    observation = renderer.build_generation_prompt(messages)
    stop_condition = renderer.get_stop_sequences()

    completer = TinkerTokenCompleter(
        sampling_client,
        max_tokens=max_tokens,
        temperature=temperature,
    )
    result = await completer(observation, stop_condition)

    message, _ = renderer.parse_response(result.tokens)
    content = message.get("content", "")
    parsed = parse_structured_response(content)
    return parsed.kernel if parsed.kernel else content


async def evaluate_problem(
    sampling_client: tinker.SamplingClient,
    problem: KernelBenchProblem,
    renderer: renderers.Renderer,
    cfg: EvalConfig,
) -> EvalResult:
    """Evaluate a single problem with multiple samples."""
    samples = []

    for sample_idx in range(cfg.num_samples):
        # Generate kernel
        kernel_code = await generate_kernel(
            sampling_client,
            problem,
            renderer,
            cfg.max_tokens,
            cfg.temperature if cfg.num_samples == 1 else 1.0,  # Use temp=1 for multiple samples
        )

        # Evaluate
        eval_result = await evaluate_kernel_async(
            level=problem.level,
            problem_id=problem.problem_id,
            backend=problem.backend,
            kernel_code=kernel_code,
            dataset_src=problem.dataset_src,
            num_correct_trials=cfg.num_correct_trials,
            measure_performance=cfg.measure_performance,
            num_perf_trials=cfg.num_perf_trials,
            timing_method=cfg.timing_method,
            precision=cfg.precision,
            check_for_excessive_speedup=cfg.check_for_excessive_speedup,
            excessive_speedup_threshold=cfg.excessive_speedup_threshold,
            timeout=cfg.modal_timeout,
        )

        if cfg.max_kernel_code_chars is None:
            kernel_code_logged = kernel_code
        elif len(kernel_code) > cfg.max_kernel_code_chars:
            kernel_code_logged = kernel_code[: cfg.max_kernel_code_chars] + "..."
        else:
            kernel_code_logged = kernel_code

        samples.append({
            "sample_id": sample_idx,
            "kernel_code": kernel_code_logged,
            **eval_result,
        })

    correct_samples = [s for s in samples if s.get("correctness")]
    if correct_samples:
        best = max(correct_samples, key=_speedup_value)
    else:
        # Best by compilation
        compiled = [s for s in samples if s.get("compiled")]
        best = compiled[0] if compiled else samples[0]

    best_speedup: float | None = None
    speedup_obj = best.get("speedup")
    if isinstance(speedup_obj, (int, float)):
        best_speedup = float(speedup_obj)

    return EvalResult(
        level=problem.level,
        problem_id=problem.problem_id,
        samples=samples,
        best_correct=bool(best.get("correctness")),
        best_compiled=bool(best.get("compiled")),
        best_speedup=best_speedup,
    )


async def evaluate_problem_multiturn(
    sampling_client: tinker.SamplingClient,
    problem: KernelBenchProblem,
    renderer: renderers.Renderer,
    cfg: EvalConfig,
    tokenizer: object | None = None,
) -> EvalResult:
    """Evaluate a single problem using multi-turn refinement.

    Runs up to ``cfg.multiturn_max_turns`` turns.  Each turn generates a
    kernel, evaluates it, and feeds back the result as context for the
    next turn.  The best result across all turns is returned.

    When ``cfg.kevin_prompt`` is True, uses Kevin's exact prompt format,
    think token injection, and Kevin-style response parsing.
    """
    from kernelbench_tinker.envs.multiturn_kernelbench_env import (
        build_eval_feedback,
        build_kevin_prompt,
        build_kevin_base_prompt,
        extract_raw_content,
    )
    from kernelbench_tinker.envs.kernelbench_env import build_system_prompt
    from kernelbench_tinker.envs.kernelbench_client import get_reference_code

    samples: list[dict[str, Any]] = []
    history: list[dict[str, str]] = []  # [{raw_content, feedback}]

    system_prompt = build_system_prompt(problem.backend, multiturn=True)

    # Pre-build Kevin prompts if enabled
    kevin_turn0_prompt = None
    kevin_base_prompt = None
    if cfg.kevin_prompt:
        ref_code = get_reference_code(
            problem.level, problem.problem_id, problem.dataset_src
        )
        kevin_turn0_prompt = build_kevin_prompt(ref_code)
        kevin_base_prompt = build_kevin_base_prompt(ref_code)

    # Think token injection setup
    think_chunk = None
    if cfg.inject_think_token and tokenizer is not None:
        from tinker.types.model_input_chunk import EncodedTextChunk
        think_ids = tokenizer.encode("<think>\n", add_special_tokens=False)
        think_chunk = EncodedTextChunk(tokens=think_ids)

    for turn in range(cfg.multiturn_max_turns):
        # Build messages
        messages: list[dict[str, str]] = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})

        if turn == 0:
            if cfg.kevin_prompt:
                messages.append({"role": "user", "content": kevin_turn0_prompt})
            else:
                messages.append({"role": "user", "content": problem.prompt})
        else:
            if cfg.kevin_prompt:
                base = kevin_base_prompt
            else:
                base = problem.base_prompt
            messages.append({
                "role": "user",
                "content": base + "Here are your previous attempts:\n",
            })
            for entry in history:
                messages.append({"role": "assistant", "content": entry["raw_content"]})
                messages.append({"role": "user", "content": entry["feedback"]})

        # Generate
        observation = renderer.build_generation_prompt(messages)
        if think_chunk is not None:
            observation = observation.append(think_chunk)
        stop_condition = renderer.get_stop_sequences()
        completer = KernelBenchTokenCompleter(
            sampling_client,
            max_tokens=cfg.max_tokens,
            temperature=cfg.temperature if cfg.num_samples == 1 else 1.0,
            top_p=cfg.top_p,
            seed=cfg.seed,
        )
        result = await completer(observation, stop_condition)
        message, _ = renderer.parse_response(result.tokens)
        response_text = message.get("content", "")

        if cfg.kevin_prompt:
            parsed = parse_kevin_response(response_text)
        else:
            parsed = parse_structured_response(response_text)
        kernel_code = parsed.kernel if parsed.kernel else response_text

        # Evaluate
        eval_result = await evaluate_kernel_async(
            level=problem.level,
            problem_id=problem.problem_id,
            backend=problem.backend,
            kernel_code=kernel_code,
            dataset_src=problem.dataset_src,
            num_correct_trials=cfg.num_correct_trials,
            measure_performance=cfg.measure_performance,
            num_perf_trials=cfg.num_perf_trials,
            timing_method=cfg.timing_method,
            precision=cfg.precision,
            check_for_excessive_speedup=cfg.check_for_excessive_speedup,
            excessive_speedup_threshold=cfg.excessive_speedup_threshold,
            timeout=cfg.modal_timeout,
        )

        # Build feedback for next turn
        feedback = build_eval_feedback(eval_result)

        eos_token = getattr(tokenizer, "eos_token", None) if tokenizer else None
        raw_content = extract_raw_content(response_text, eos_token)
        history.append({"raw_content": raw_content, "feedback": feedback})

        if cfg.max_kernel_code_chars is None:
            kernel_code_logged = kernel_code
        elif len(kernel_code) > cfg.max_kernel_code_chars:
            kernel_code_logged = kernel_code[: cfg.max_kernel_code_chars] + "..."
        else:
            kernel_code_logged = kernel_code

        samples.append({
            "sample_id": turn,
            "turn": turn,
            "kernel_code": kernel_code_logged,
            **eval_result,
        })

    correct_samples = [s for s in samples if s.get("correctness")]
    if correct_samples:
        best = max(correct_samples, key=_speedup_value)
    else:
        compiled = [s for s in samples if s.get("compiled")]
        best = compiled[0] if compiled else samples[0]

    best_speedup: float | None = None
    speedup_obj = best.get("speedup")
    if isinstance(speedup_obj, (int, float)):
        best_speedup = float(speedup_obj)

    return EvalResult(
        level=problem.level,
        problem_id=problem.problem_id,
        samples=samples,
        best_correct=bool(best.get("correctness")),
        best_compiled=bool(best.get("compiled")),
        best_speedup=best_speedup,
    )


async def run_evaluation(cfg: EvalConfig) -> dict[str, Any]:
    """Run full evaluation."""
    from kernelbench_tinker.modal.evaluator import (
        ModalEvaluatorConfig,
        ModalKernelEvaluator,
        set_modal_evaluator,
    )

    modal_config = ModalEvaluatorConfig(
        enabled=True,
        gpu_type=cfg.modal_gpu_type,
        timeout=int(cfg.modal_timeout),
    )
    set_modal_evaluator(ModalKernelEvaluator(modal_config))

    # Create Tinker client
    service_client = tinker.ServiceClient(base_url=cfg.base_url)

    # Load checkpoint
    if cfg.checkpoint_path:
        logger.info(f"Loading checkpoint: {cfg.checkpoint_path}")
        sampling_client = service_client.create_sampling_client(cfg.checkpoint_path)
    else:
        # Use base model
        logger.info(f"Using base model: {cfg.model_name}")
        sampling_client = service_client.create_sampling_client(base_model=cfg.model_name)

    # Get renderer
    renderer_name = get_renderer_name_for_model(cfg.model_name)
    tokenizer = tokenizer_utils.get_tokenizer(cfg.model_name)
    renderer = renderers.get_renderer(renderer_name, tokenizer)

    # Get problems
    problem_ids = get_problem_ids(
        cfg.level,
        start=cfg.start_problem,
        end=cfg.end_problem,
        dataset_src=cfg.dataset_src,
    )

    problems = [
        KernelBenchProblem(
            level=cfg.level,
            problem_id=pid,
            backend=cfg.backend,
            dataset_src=cfg.dataset_src,
            prompt_option=cfg.prompt_option,
            prompt_precision=cfg.precision,
            prompt_include_hardware=cfg.prompt_include_hardware,
            prompt_gpu_name=cfg.prompt_gpu_name,
        )
        for pid in problem_ids
    ]

    logger.info(f"Evaluating {len(problems)} problems from level {cfg.level}")

    # Evaluate each problem
    results = []
    for problem in tqdm(problems, desc="Evaluating"):
        try:
            if cfg.multiturn_enabled:
                result = await evaluate_problem_multiturn(
                    sampling_client, problem, renderer, cfg, tokenizer=tokenizer
                )
            else:
                result = await evaluate_problem(
                    sampling_client, problem, renderer, cfg
                )
            results.append(result)
        except Exception as e:
            logger.error(f"Error evaluating problem {problem.problem_id}: {e}")
            results.append(EvalResult(
                level=problem.level,
                problem_id=problem.problem_id,
                samples=[{"error": str(e)}],
                best_correct=False,
                best_compiled=False,
                best_speedup=None,
            ))

    # Compute aggregate metrics
    num_correct = sum(1 for r in results if r.best_correct)
    num_compiled = sum(1 for r in results if r.best_compiled)
    speedups = [r.best_speedup for r in results if r.best_speedup is not None]

    metrics = {
        "num_problems": len(results),
        "num_correct": num_correct,
        "num_compiled": num_compiled,
        "correct_rate": num_correct / len(results) if results else 0,
        "compile_rate": num_compiled / len(results) if results else 0,
        "mean_speedup": sum(speedups) / len(speedups) if speedups else None,
        "num_with_speedup": len(speedups),
    }

    # Build full output
    output = {
        "config": {
            "checkpoint_path": cfg.checkpoint_path,
            "model_name": cfg.model_name,
            "level": cfg.level,
            "backend": cfg.backend,
            "num_samples": cfg.num_samples,
        },
        "metrics": metrics,
        "results": [asdict(r) for r in results],
    }

    return output


def main():
    """Main entry point."""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )
    setup_environment()

    # Parse CLI arguments
    cfg = chz.entrypoint(EvalConfig)

    logger.info("Starting KernelBench Evaluation")
    logger.info(f"Checkpoint: {cfg.checkpoint_path or 'base model'}")
    logger.info(f"Level: {cfg.level}")
    logger.info(f"Backend: {cfg.backend}")

    # Run evaluation
    output = asyncio.run(run_evaluation(cfg))

    # Print summary
    metrics = output["metrics"]
    logger.info("=" * 50)
    logger.info("Evaluation Results:")
    logger.info(f"  Problems: {metrics['num_problems']}")
    logger.info(f"  Compiled: {metrics['num_compiled']} ({metrics['compile_rate']:.1%})")
    logger.info(f"  Correct:  {metrics['num_correct']} ({metrics['correct_rate']:.1%})")
    if metrics.get("mean_speedup"):
        logger.info(f"  Mean Speedup: {metrics['mean_speedup']:.2f}x")
    logger.info("=" * 50)

    # Log to TensorBoard if specified
    if cfg.tensorboard_log_dir:
        # Lazy import to avoid circular imports
        from kernelbench_tinker.training.tensorboard_logger import create_tensorboard_logger
        from kernelbench_tinker.evaluation.eval_kernelbench import EvalResults, ProblemResult

        tb_logger = create_tensorboard_logger(cfg.tensorboard_log_dir)

        # Create EvalResults for computing summary
        eval_results = EvalResults()
        for result_data in output["results"]:
            problem_result = ProblemResult(
                level=result_data["level"],
                problem_id=result_data["problem_id"],
                samples=result_data["samples"],
            )
            eval_results.add_problem(problem_result)

        # Log evaluation summary
        summary = eval_results.summary()
        tb_logger.log_evaluation_metrics(summary, cfg.tensorboard_step, prefix=f"Eval/Level{cfg.level}")

        tb_logger.flush()
        tb_logger.close()
        logger.info(f"Logged evaluation metrics to TensorBoard at step {cfg.tensorboard_step}")

    # Save results
    os.makedirs(os.path.dirname(cfg.output_path) or ".", exist_ok=True)
    with open(cfg.output_path, "w") as f:
        json.dump(output, f, indent=2, default=str)
    logger.info(f"Results saved to {cfg.output_path}")


if __name__ == "__main__":
    main()
