"""
Reward shaping for KernelBench RL training.

This module implements reward functions that combine:
- Format correctness (valid code block extraction)
- Compilation success
- Correctness (passing all tests)
- Speed (optional, for later stages of training)
"""

from __future__ import annotations

import math
from dataclasses import dataclass
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from kernelbench_tinker.envs.kernelbench_client import KernelEvalResult


@dataclass
class RewardConfig:
    """
    Configuration for reward computation.

    Default values follow the published KernelBench RL setup:
    - Reward = 0.3 * correct + speedup (if correct)
    - No length penalties (can cause response collapse)
    - Binary correctness (no partial credit)
    """

    # ==========================================================================
    # Default reward weights
    # Formula: S = 0.3 * correct + (T_baseline/T_kernel) * correct
    # ==========================================================================
    format_weight: float = 0.0  # Disabled by default
    compile_weight: float = 0.0  # Disabled by default
    correctness_weight: float = 0.3  # Binary correctness weight
    speed_weight: float = 1.0  # Adds speedup directly
    length_weight: float = 0.0  # Length penalties can cause response collapse

    format_penalty: float = 0.0  # Zero reward for bad format

    # ==========================================================================
    # Speed reward configuration
    # Linear speedup: reward = T_baseline / T_kernel
    # ==========================================================================
    speed_baseline: float = 1.0  # Speedup threshold (1.0 = same as baseline)
    speed_scale: float = 1.0  # Linear scaling (not log)
    speed_max_reward: float = 10.0  # Cap to prevent outliers

    # ==========================================================================
    # Length reward configuration (DISABLED by default)
    # Length penalties can cause response collapse.
    # ==========================================================================
    length_max: int = 8000  # Not used when length_weight=0
    length_min: int = 500  # Not used when length_weight=0

    # ==========================================================================

    # ==========================================================================
    # Correctness configuration
    # Binary correctness (all tests pass or not)
    # ==========================================================================
    sparse_rewards: bool = False
    partial_correctness: bool = False  # Binary correctness


def format_reward(eval_result: "KernelEvalResult", config: RewardConfig) -> float:
    """
    Compute reward for format correctness.

    Returns:
        1.0 if format is valid (code block with correct language tag)
        config.format_penalty if format is invalid
    """
    if eval_result["format_ok"]:
        return 1.0
    return config.format_penalty


def compile_reward(eval_result: "KernelEvalResult", config: RewardConfig) -> float:
    """
    Compute reward for successful compilation.

    Returns:
        1.0 if compiled successfully
        0.0 if compilation failed
    """
    if eval_result["compiled"]:
        return 1.0

    return 0.0


def correctness_reward(eval_result: "KernelEvalResult", config: RewardConfig) -> float:
    """
    Compute reward for correctness.

    With partial_correctness=True:
        Returns tests_passed / tests_total (0.0 to 1.0)

    With partial_correctness=False:
        Returns 1.0 if all tests pass, 0.0 otherwise
    """
    if not eval_result["compiled"]:
        return 0.0

    tests_passed = eval_result["tests_passed"]
    tests_total = eval_result["tests_total"]

    if tests_total == 0:
        return 0.0

    if config.partial_correctness:
        return tests_passed / tests_total
    else:
        return 1.0 if tests_passed == tests_total else 0.0


def speed_reward(
    eval_result: "KernelEvalResult",
    config: RewardConfig,
    use_speed: bool = True
) -> float:
    """
    Compute reward for speedup over baseline.

    Formula:
        speed_reward = T_baseline / T_kernel = speedup

    Uses LINEAR speedup directly, not log-scaled.
    With speed_scale=1.0 (default), this returns the raw speedup value.

    Only gives reward if:
    - use_speed is True
    - Kernel is fully correct
    - Speedup data is available

    Args:
        eval_result: Evaluation result
        config: Reward configuration
        use_speed: Whether to use speed rewards

    Returns:
        Speed reward (0.0 if not applicable)
    """
    if not use_speed:
        return 0.0

    # Only reward speed for fully correct kernels
    if not eval_result["correctness"]:
        return 0.0

    speedup = eval_result.get("speedup")
    if speedup is None or speedup <= 0:
        return 0.0

    # Linear speedup, not log-scaled
    # If speedup <= baseline (1.0), no speed bonus
    if speedup <= config.speed_baseline:
        return 0.0

    # Linear reward: speedup - 1.0 (so 2x speedup = 1.0 reward, 3x = 2.0, etc.)
    # This matches the default formula where reward = speedup for correct kernels
    reward = config.speed_scale * (speedup - config.speed_baseline)

    # Clamp to max to prevent outliers
    return min(reward, config.speed_max_reward)


def length_reward(eval_result: "KernelEvalResult", config: RewardConfig) -> float:
    """
    Compute reward based on code length (GRPO-LEAD style tie-breaking).

    This provides a continuous signal that breaks ties when other rewards are uniform.
    Shorter code gets higher reward, encouraging concise solutions.

    The reward is linearly interpolated:
    - Code <= length_min: reward = 1.0
    - Code >= length_max: reward = 0.0
    - In between: linear interpolation

    Args:
        eval_result: Evaluation result containing code_length
        config: Reward configuration

    Returns:
        Length reward in [0, 1]
    """
    code_length = eval_result.get("code_length", 0)

    if code_length <= config.length_min:
        return 1.0
    elif code_length >= config.length_max:
        return 0.0
    else:
        # Linear interpolation
        range_size = config.length_max - config.length_min
        position = code_length - config.length_min
        return 1.0 - (position / range_size)




def compute_reward(
    eval_result: "KernelEvalResult",
    config: RewardConfig | None = None,
) -> float:
    """
    Compute the total reward for a kernel evaluation.

    Formula:
        S = 0.3 * correct + (T_baseline/T_kernel) * correct

    Key behaviors:
    - Zero reward for incorrect kernels
    - Binary correctness (no partial credit by default)
    - Speedup added linearly (not log-scaled)

    Args:
        eval_result: Result from kernel evaluation
        config: Reward configuration (uses defaults if None)

    Returns:
        Total reward (scalar)
    """
    if config is None:
        config = RewardConfig()

    # ==========================================================================
    # Zero reward for bad format
    # ==========================================================================
    if not eval_result["format_ok"]:
        return 0.0

    # Sparse reward mode: only reward fully correct solutions
    if config.sparse_rewards:
        if eval_result["correctness"]:
            base_reward = 1.0
            # Add speed bonus if enabled
            if config.speed_weight > 0:
                s_reward = speed_reward(eval_result, config, use_speed=True)
                base_reward += config.speed_weight * s_reward
            # Add length bonus for tie-breaking (disabled by default)
            if config.length_weight > 0:
                l_reward = length_reward(eval_result, config)
                base_reward += config.length_weight * l_reward
            return base_reward
        return 0.0

    # ==========================================================================
    # Default reward computation
    # Formula: S = correctness_weight * correct + speed_weight * speedup
    # With default weights: S = 0.3 * correct + 1.0 * speedup
    # ==========================================================================

    # Correctness reward (binary by default)
    corr_reward = correctness_reward(eval_result, config)

    # If not correct, zero reward
    if corr_reward == 0.0:
        return 0.0

    # Base reward for correctness
    total = config.correctness_weight * corr_reward

    # Add speedup reward (only for correct kernels)
    if config.speed_weight > 0:
        s_reward = speed_reward(eval_result, config, use_speed=True)
        total += config.speed_weight * s_reward

    # Optional: format/compile rewards (disabled by default)
    if config.format_weight > 0:
        f_reward = format_reward(eval_result, config)
        total += config.format_weight * f_reward

    if config.compile_weight > 0:
        c_reward = compile_reward(eval_result, config)
        total += config.compile_weight * c_reward

    # Optional: length reward (DISABLED by default)
    if config.length_weight > 0:
        l_reward = length_reward(eval_result, config)
        total += config.length_weight * l_reward


    return total


def compute_reward_breakdown(
    eval_result: "KernelEvalResult",
    config: RewardConfig | None = None,
) -> dict[str, float]:
    """
    Compute individual reward components for logging.

    Args:
        eval_result: Result from kernel evaluation
        config: Reward configuration (uses defaults if None)

    Returns:
        Dictionary with individual reward values
    """
    if config is None:
        config = RewardConfig()

    return {
        "reward_format": format_reward(eval_result, config),
        "reward_compile": compile_reward(eval_result, config),
        "reward_correctness": correctness_reward(eval_result, config),
        "reward_speed": speed_reward(eval_result, config, use_speed=True),
        "reward_length": length_reward(eval_result, config),
        "reward_total": compute_reward(eval_result, config),
    }

# Preset reward configurations for different training stages

def get_stage1_config() -> RewardConfig:
    """
    Stage 1: Focus on format and compilation.

    Good for early training when the model is learning
    to generate valid code structure.
    """
    return RewardConfig(
        format_weight=0.3,
        compile_weight=0.5,
        correctness_weight=0.2,
        speed_weight=0.0,
        partial_correctness=True,
        sparse_rewards=False,
    )


def get_stage2_config() -> RewardConfig:
    """
    Stage 2: Focus on correctness.

    For mid-training when the model can compile code
    but needs to learn correctness.
    """
    return RewardConfig(
        format_weight=0.1,
        compile_weight=0.2,
        correctness_weight=0.7,
        speed_weight=0.0,
        partial_correctness=True,
        sparse_rewards=False,
    )


def get_stage3_config() -> RewardConfig:
    """
    Stage 3: Correctness + Speed.

    For late training when the model can produce correct
    kernels and should optimize for speed.
    """
    return RewardConfig(
        format_weight=0.05,
        compile_weight=0.1,
        correctness_weight=0.5,
        speed_weight=0.35,
        partial_correctness=False,
        sparse_rewards=False,
    )


def get_sparse_config() -> RewardConfig:
    """
    Sparse rewards: Only reward fully correct solutions.

    May be useful for fine-tuning or when combined with
    curriculum learning.
    """
    return RewardConfig(
        sparse_rewards=True,
        speed_weight=0.5,
    )


REWARD_PRESETS = {
    "stage1": get_stage1_config,
    "stage2": get_stage2_config,
    "stage3": get_stage3_config,
    "sparse": get_sparse_config,
    "default": RewardConfig,
}


def get_reward_config(name: str) -> RewardConfig:
    """Get a reward configuration by name."""
    if name not in REWARD_PRESETS:
        raise ValueError(
            f"Unknown reward preset: {name}. Available: {list(REWARD_PRESETS.keys())}"
        )
    return REWARD_PRESETS[name]()

