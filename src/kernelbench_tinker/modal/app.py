"""
Modal App definition for isolated GPU kernel evaluation.

This module defines the Modal App, GPU image, and evaluation function
that runs inside the isolated Modal container.
"""

from __future__ import annotations

import os
import re
from typing import Any

import modal

# GPU architecture mapping for Triton/CUDA compilation
GPU_ARCH_MAPPING = {
    "A100": ["Ampere"],
    "A100-40GB": ["Ampere"],
    "A100-80GB": ["Ampere"],
    "H100": ["Hopper"],
    "H200": ["Hopper"],
    "L40S": ["Ada"],
    "L4": ["Ada"],
    "T4": ["Turing"],
    "A10G": ["Ampere"],
}

# Default configuration
DEFAULT_GPU = "A100"
DEFAULT_TIMEOUT = 120  # 2 minutes per kernel by default (tunable via config)

# KernelBench install (default: pip install pinned git rev)
KERNELBENCH_GIT_SPEC = os.environ.get(
    "KERNELBENCH_GIT_SPEC",
    "kernelbench @ git+https://github.com/ScalingIntelligence/KernelBench.git@06e49e8781b13acdb68b102b3c9c864bd9454db3",
)

# Build the Modal image with CUDA and KernelBench dependencies
cuda_version = "12.8.0"
flavor = "devel"  # includes full CUDA toolkit
operating_sys = "ubuntu22.04"
tag = f"{cuda_version}-{flavor}-{operating_sys}"

# Create image with CUDA, compilers, and Python dependencies
image = (
    modal.Image.from_registry(f"nvidia/cuda:{tag}", add_python="3.10")
    .apt_install(
        "git",
        "gcc-10",
        "g++-10",
        "clang",
    )
    .pip_install(
        KERNELBENCH_GIT_SPEC,
        "python-dotenv",
        "tqdm",
        "openai",
        "litellm[proxy]",
        "requests",
        "pydantic",
        "numpy",
        "torch==2.9.0",
        "transformers",
        "datasets",
        "einops",
        "packaging",
    )
)

# Create Modal App
app = modal.App("kernel-rl-evaluator")


@app.cls(
    image=image,
    gpu="A100",
    timeout=DEFAULT_TIMEOUT,
    max_containers=32,
    retries=modal.Retries(
        max_retries=3,
        backoff_coefficient=2.0,
        initial_delay=1.0,
    ),
    min_containers=0,
)
class KernelEvaluator:
    """Modal class for kernel evaluation with configurable GPU."""

    @modal.method()
    def evaluate(
        self,
        ref_code: str,
        kernel_code: str,
        backend: str,
        num_correct_trials: int,
        measure_performance: bool,
        num_perf_trials: int,
        gpu_arch: list[str],
        precision: str,
        timing_method: str,
        check_for_excessive_speedup: bool,
        excessive_speedup_threshold: float,
    ) -> dict[str, Any]:
        """
        Evaluate a single kernel in an isolated Modal container.

        Args:
            ref_code: Reference PyTorch implementation
            kernel_code: Generated kernel code to evaluate
            backend: Backend type ("triton", "cuda", "cute", "tilelang")
            num_correct_trials: Number of correctness trials
            measure_performance: Whether to measure runtime
            num_perf_trials: Number of performance trials
            gpu_arch: GPU architecture list for compilation
            precision: Precision string ("fp32", "fp16", "bf16")

        Returns:
            Dict matching KernelEvalResult structure
        """
        import tempfile
        from kernelbench.eval import eval_kernel_against_ref, get_torch_dtype_from_string
        from kernelbench.utils import set_gpu_arch
        import torch
        import time
        import modal.experimental

        max_wait_time = 30
        start_time = time.time()
        gpu_available = False

        while time.time() - start_time < max_wait_time:
            if torch.cuda.is_available():
                gpu_available = True
                break
            wait_time = min(0.5 * (2 ** int((time.time() - start_time) / 2)), 8.0)
            time.sleep(wait_time)

        if not gpu_available:
            raise RuntimeError(
                f"GPU not attached to container after {max_wait_time}s - Modal will retry with new container"
            )

        # Set GPU architecture for Triton/CUDA compilation
        set_gpu_arch(gpu_arch)

        try:
            # Isolate torch extension builds to avoid lock contention across evals.
            # This is critical for CUDA inline compilation in parallel Modal workers.
            build_dir = tempfile.mkdtemp(prefix="torch_ext_")
            os.environ["TORCH_EXTENSIONS_DIR"] = build_dir

            eval_kwargs = {
                "original_model_src": ref_code,
                "custom_model_src": kernel_code,
                "measure_performance": measure_performance,
                "verbose": False,
                "num_correct_trials": num_correct_trials,
                "num_perf_trials": num_perf_trials,
                "backend": backend,
                "precision": get_torch_dtype_from_string(precision),
                "timing_method": timing_method,
                "check_for_excessive_speedup": check_for_excessive_speedup,
                "excessive_speedup_threshold": excessive_speedup_threshold,
                "build_dir": build_dir,
            }
            try:
                result = eval_kernel_against_ref(**eval_kwargs)
            except TypeError as exc:
                msg = str(exc)
                if "timing_method" in msg:
                    eval_kwargs.pop("timing_method", None)
                if "check_for_excessive_speedup" in msg:
                    eval_kwargs.pop("check_for_excessive_speedup", None)
                if "excessive_speedup_threshold" in msg:
                    eval_kwargs.pop("excessive_speedup_threshold", None)
                if "build_dir" in msg:
                    eval_kwargs.pop("build_dir", None)
                if msg == str(exc) and "timing_method" not in msg and "check_for_excessive_speedup" not in msg and "excessive_speedup_threshold" not in msg and "build_dir" not in msg:
                    raise
                result = eval_kernel_against_ref(**eval_kwargs)
            torch.cuda.empty_cache()

            if result is None:
                return {
                    "format_ok": True,
                    "compiled": False,
                    "correctness": False,
                    "tests_passed": 0,
                    "tests_total": num_correct_trials,
                    "speedup": None,
                    "runtime_ms": None,
                    "baseline_runtime_ms": None,
                    "error_message": "Evaluation returned None (lock file error)",
                    "code_length": len(kernel_code),
                    "metadata": {},
                }

            # Parse tests passed from metadata
            tests_passed = 0
            trials_str = result.metadata.get("correctness_trials", "(0 / 0)")
            match = re.match(r"\((\d+)\s*/\s*(\d+)\)", trials_str)
            if match:
                tests_passed = int(match.group(1))

            runtime_ms = result.runtime if result.runtime > 0 else None
            baseline_runtime_ms = (
                result.metadata.get("baseline_runtime_ms")
                or result.metadata.get("baseline_runtime")
                or None
            )
            speedup = None
            if (
                result.correctness
                and runtime_ms is not None
                and baseline_runtime_ms
                and baseline_runtime_ms > 0
            ):
                speedup = baseline_runtime_ms / runtime_ms

            # Extract error message if any
            error_message = None
            for key in ["runtime_error", "compilation_error", "correctness_issue"]:
                if key in result.metadata:
                    error_message = str(result.metadata[key])
                    break

            return {
                "format_ok": True,
                "compiled": result.compiled,
                "correctness": result.correctness,
                "tests_passed": tests_passed,
                "tests_total": num_correct_trials,
                "speedup": speedup,
                "runtime_ms": runtime_ms,
                "baseline_runtime_ms": baseline_runtime_ms,
                "error_message": error_message,
                "code_length": len(kernel_code),
                "metadata": dict(result.metadata),
            }

        except torch.cuda.CudaError as e:
            modal.experimental.stop_fetching_inputs()
            return {
                "format_ok": True,
                "compiled": False,
                "correctness": False,
                "tests_passed": 0,
                "tests_total": num_correct_trials,
                "speedup": None,
                "runtime_ms": None,
                "baseline_runtime_ms": None,
                "error_message": f"Modal evaluation failed: {str(e)}",
                "code_length": len(kernel_code),
                "metadata": {"gpu_error": type(e).__name__, "error_message": str(e)[:500]},
            }
        except Exception as e:
            return {
                "format_ok": True,
                "compiled": False,
                "correctness": False,
                "tests_passed": 0,
                "tests_total": num_correct_trials,
                "speedup": None,
                "runtime_ms": None,
                "baseline_runtime_ms": None,
                "error_message": f"Modal evaluation failed: {str(e)}",
                "code_length": len(kernel_code),
                "metadata": {"exception": str(e), "exception_type": type(e).__name__},
            }


def get_gpu_arch(gpu_type: str) -> list[str]:
    """Get GPU architecture list for a given GPU type."""
    return GPU_ARCH_MAPPING.get(gpu_type, ["Ampere"])
