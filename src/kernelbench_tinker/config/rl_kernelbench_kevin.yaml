# KernelBench RL Training Configuration — Kevin Reproduction
# ===========================================================
#
# Faithfully reproduces the Kevin multi-turn RL training setup.
# Key differences from rl_kernelbench.yaml:
#   - model: QwQ-32B (not Qwen3-30B-A3B)
#   - backend: cuda (not triton)
#   - gamma: 0.8 (not 0.4)
#   - num_correct_trials: 1 (not 5)
#   - num_perf_trials: 10 (not 100)
#   - reward_clip: [-10, 10]
#   - speed_max_reward: uncapped (100000.0)
#   - modal_timeout: 300s (not 60s)
#   - num_epochs: 6 (Kevin: num_episodes=6)
#   - check_for_excessive_speedup: false
#   - prompt_max_tokens: 8192 (token-based history truncation)
#   - multi-turn enabled with 4 turns
#
# Known gap: Kevin uses top_p=0.95 which TinkerTokenCompleter does not expose.
#
# Usage:
#   python -m kernelbench_tinker.scripts.train_kernel_rl \
#       --config src/kernelbench_tinker/config/rl_kernelbench_kevin.yaml \
#       log_path=./runs/kevin_reproduction

# =============================================================================
# Model Configuration
# =============================================================================
model_name: "Qwen/QwQ-32B"
lora_rank: 64
learning_rate: 0.000002

# =============================================================================
# Generation Configuration
# =============================================================================
max_tokens: 16384
temperature: 0.9
max_tokens_extended: 22528          # Kevin: extend to 22K mid-training
max_tokens_extend_after_step: 30    # Kevin: extend at step 30

# =============================================================================
# Multi-turn Configuration
# =============================================================================
multiturn:
    enabled: true
    n: 4                               # 4 refinement turns
    gamma: 0.8                         # Kevin's discount factor
    aggregation: "sum"
    early_stop_on_correct: false
    speedup_threshold: null

# =============================================================================
# Training Configuration
# =============================================================================
num_substeps: 2
loss_fn: "ppo"
max_grad_norm: 0.05
warmup_ratio: 0.03
clip_epsilon_low: 0.2
clip_epsilon_high: 0.28
constant_length_norm: 16384
kl_penalty_coef: 0.0
kl_discount_factor: 0.0
remove_constant_reward_groups: true

# =============================================================================
# Logging and Checkpointing
# =============================================================================
log_path: "./runs/kevin_reproduction"
save_every: 1
wandb_project: kernelbench-tinker-kevin
wandb_name: kevin-repro

# =============================================================================
# Dataset Builder Configuration
# =============================================================================
dataset_builder:
    level: 1
    start_problem: null
    end_problem: null
    dataset_src: "huggingface"
    backend: "cuda"                   # Kevin uses CUDA backend

    batch_size: 8                     # Kevin: rollout_batch_size=8
    group_size: 16                    # Kevin: num_samples_per_prompt=64, num_passes=4 => 16
    num_epochs: 6                     # Kevin: num_episodes=6
    shuffle: true
    test_fraction: 0.1

    renderer_name: "qwen3"
    prompt_option: "one_shot"
    prompt_precision: null
    prompt_include_hardware: false
    prompt_gpu_name: null
    prompt_max_tokens: 8192           # Kevin: prompt_max_len=8192 for token-based truncation

    # Evaluation — Kevin's lightweight settings
    num_correct_trials: 1             # Kevin: 1 trial
    measure_performance: true
    num_perf_trials: 10               # Kevin: 10 trials
    timing_method: "cuda_event"
    precision: "fp32"
    check_for_excessive_speedup: false  # Kevin has no excessive speedup check
    excessive_speedup_threshold: 10.0
    modal_gpu_type: "A100"
    modal_timeout: 300.0              # Kevin: 5 minutes

    # Reward — Kevin's reward shaping
    reward_format_weight: 0.0
    reward_compile_weight: 0.0
    reward_correctness_weight: 0.3
    reward_speed_weight: 1.0
    reward_length_weight: 0.0
    reward_speed_max_reward: 100000.0  # Kevin: no speed cap (uncapped speedup flows to clipping)
    reward_clip_min: -10.0            # Kevin: clip total reward to [-10, 10]
    reward_clip_max: 10.0

    # Reward hacking detection
    reward_enable_static_checker: true
    reward_static_checker_backend: "cuda"
    reward_static_checker_precision: "fp32"
    reward_static_checker_strict: null
    reward_static_checker_warnings: null
