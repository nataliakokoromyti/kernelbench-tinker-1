# KernelBench RL Training Configuration (Integration Default)
# =============================================================
#
# Default config for the KernelBench â†” Tinker integration.
#
# Required Environment Variables:
#   TINKER_API_KEY       - Tinker distributed training API key
#   MODAL_TOKEN_ID       - Modal API token ID (for isolated GPU evaluation)
#   MODAL_TOKEN_SECRET   - Modal API token secret
#
# Usage:
#   python -m kernelbench_tinker.scripts.train_kernel_rl \
#       --config src/kernelbench_tinker/config/rl_kernelbench.yaml \
#       log_path=./runs/my_experiment

# =============================================================================
# Model Configuration
# =============================================================================
model_name: "Qwen/Qwen3-30B-A3B"
lora_rank: 64
learning_rate: 0.000002  # 2e-6 as explicit float

# =============================================================================
# Generation Configuration
# =============================================================================
max_tokens: 16384
temperature: 1.0

# =============================================================================
# Training Mode
# =============================================================================
# "single_turn" (default) or "multi_turn"
mode: "single_turn"

# Multi-turn settings (only used when mode: "multi_turn")
gamma: 0.4                       # Discount factor for multi-turn returns
n: 4                             # Refinement turns per trajectory
m: 16                            # Parallel trajectories per problem

# =============================================================================
# Training Configuration
# =============================================================================
num_substeps: 2
loss_fn: "importance_sampling"

# KL Regularization (optional)
kl_penalty_coef: 0.0
kl_discount_factor: 0.0

# Remove groups with constant rewards (no learning signal)
remove_constant_reward_groups: true

# =============================================================================
# Logging and Checkpointing
# =============================================================================
log_path: "./runs/default"
save_every: 1

# Weights & Biases (optional)
wandb_project: kernelbench-tinker
wandb_name: kb-tinker-rl

# =============================================================================
# Dataset Builder Configuration
# =============================================================================
dataset_builder:
    # ---------------------------------------------------------------------------
    # Problem Selection
    # ---------------------------------------------------------------------------
    level: 1                      # KernelBench level (1, 2, 3, or 4)
    start_problem: null           # First problem ID (null = start from 1)
    end_problem: null             # Last problem ID (null = all problems)
    dataset_src: "huggingface"    # "huggingface" or "local"

    # ---------------------------------------------------------------------------
    # Kernel Backend (what language the LLM generates)
    # Options: "triton", "cuda", "cute", "tilelang"
    # ---------------------------------------------------------------------------
    backend: "triton"

    # ---------------------------------------------------------------------------
    # Batching
    # ---------------------------------------------------------------------------
    batch_size: 8                 # Problems per batch
    group_size: 16                # Rollouts per problem (for GRPO)
    num_epochs: 22
    shuffle: true
    test_fraction: 0.1            # Fraction of problems for test set

    # ---------------------------------------------------------------------------
    # Prompt Configuration
    # ---------------------------------------------------------------------------
    renderer_name: "qwen3"        # Should match model family
    prompt_option: "one_shot"     # "zero_shot", "one_shot", "few_shot"
    prompt_precision: null
    prompt_include_hardware: false
    prompt_gpu_name: null

    # ---------------------------------------------------------------------------
    # Evaluation (runs on Modal GPU containers)
    # ---------------------------------------------------------------------------
    num_correct_trials: 5
    measure_performance: true
    num_perf_trials: 100
    timing_method: "cuda_event"
    precision: "fp32"
    check_for_excessive_speedup: true
    excessive_speedup_threshold: 10.0
    modal_gpu_type: "A100"
    modal_timeout: 60.0

    # ---------------------------------------------------------------------------
    # Reward Weights
    # Formula: reward = correctness_weight * correct + speed_weight * speedup
    # ---------------------------------------------------------------------------
    reward_format_weight: 0.0
    reward_compile_weight: 0.0
    reward_correctness_weight: 0.3
    reward_speed_weight: 1.0
    reward_length_weight: 0.0

    # ---------------------------------------------------------------------------
    # Reward Hacking Detection (Static Checker)
    # ---------------------------------------------------------------------------
    # Enable static checking for reward hacking patterns
    reward_enable_static_checker: true
    
    # Backend for static checking (must match backend above)
    reward_static_checker_backend: "triton"  # "cuda", "triton", "thunderkittens", "cute", "tilelang"
    
    # Precision for static checking (must match precision above)
    reward_static_checker_precision: "fp32"  # "fp32", "fp16", "bf16"
    
    # Strict checks (errors) - these will zero the reward if detected
    # Set to null to use all default strict checks
    # Available checks: "code_bypass", "timing_event_patch", "thread_injection", 
    #                  "lazy_eval", "cuda_impl", "triton_impl", "tk_impl", 
    #                  "cute_impl", "tilelang_impl", "pytorch_wrap"
    reward_static_checker_strict: null  # null = use defaults (all strict checks)
    
    # Warning checks - these will log warnings but NOT zero the reward
    # Set to null to use all default warning checks
    # Available checks: "pytorch_wrap", "torch_computation_ops", "stream_injection",
    #                  "precision_downgrade"
    reward_static_checker_warnings: null  # null = use defaults (all warning checks)
