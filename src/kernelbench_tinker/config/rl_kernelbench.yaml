# KernelBench RL Training Configuration (Kevin-32B Compatible)
# =============================================================
#
# Configuration aligned with Kevin-32B paper (arXiv:2507.11948):
# - Multi-turn refinement with discounted returns (gamma=0.4)
# - Kevin reward formula: S = 0.3??correct + speedup
# - No length penalties (causes response collapse)
# - No thinking rewards (thinking removed from prompts)
# - Binary correctness (no partial credit)
# - Zero reward for cheating (PyTorch wrapping, try-except, pass)
#
# Required Environment Variables:
#   TINKER_API_KEY       - Tinker distributed training API key
#   MODAL_TOKEN_ID       - Modal API token ID (for isolated GPU evaluation)
#   MODAL_TOKEN_SECRET   - Modal API token secret
#
# Usage:
#   python -m kernelbench_tinker.scripts.train_kernel_rl \
#       --config src/kernelbench_tinker/config/rl_kernelbench.yaml \
#       log_path=./runs/my_experiment

# =============================================================================
# Model Configuration
# Kevin uses QwQ-32B, we use Qwen3-30B-A3B (similar architecture)
# =============================================================================
model_name: "Qwen/Qwen3-30B-A3B"
lora_rank: 64  # Tinker max for Qwen3-30B-A3B
learning_rate: 0.000002 # Kevin uses 2e-6 (very conservative)

# =============================================================================
# Generation Configuration
# Kevin uses 16K initially, extends to 22K at step 30
# =============================================================================
max_tokens: 16384 # Kevin uses 16K-22K tokens
temperature: 1.0

# =============================================================================
# Training Mode
# =============================================================================
mode: "multi_turn" # Kevin-style multi-turn refinement

# =============================================================================
# Multi-Turn Configuration (Kevin Mode)
# =============================================================================
max_turns: 4 # Kevin uses T=4 refinement attempts
gamma: 0.4 # Kevin paper: gamma=0.4 works best

# =============================================================================
# Training Configuration
# Kevin: 2 gradient steps per batch, aggressive gradient clipping (0.05)
# =============================================================================
num_substeps: 2 # Kevin uses 2 gradient steps per batch
loss_fn: "importance_sampling"

# KL Regularization (optional)
kl_penalty_coef: 0.0
kl_discount_factor: 0.0

# Remove groups with constant rewards (no learning signal)
remove_constant_reward_groups: true

# =============================================================================
# Logging and Checkpointing
# =============================================================================
log_path: "./runs/kernelbench_tinker"
save_every: 1 # Save after every batch (kernel eval can crash GPU)
eval_every: 10

# Weights & Biases (optional)
wandb_project: null
wandb_name: null

# =============================================================================
# Dataset Configuration
# Kevin: 8 tasks per batch, 16 parallel trajectories per task
# =============================================================================
dataset_builder:
    # Problem selection
    level: 1
    start_problem: null # null = start from beginning
    end_problem: null # null = use all problems
    backend: "triton"
    dataset_src: "huggingface"

    # Training configuration
    # Kevin: 8 tasks ?? 16 trajectories = 128 samples per batch
    # Adjust based on your GPU memory
    batch_size: 8 # Kevin uses 8 tasks per batch
    group_size: 16 # Kevin uses 16 parallel trajectories per task
    num_epochs: 22
    shuffle: true

    # Evaluation configuration
    num_correct_trials: 5
    measure_performance: true # Enable for speed rewards (Kevin uses speedup)

    # ==========================================================================
    # Reward configuration (Kevin-32B compatible)
    # Kevin formula: S = 0.3??correct + speedup
    # ==========================================================================
    reward_format_weight: 0.0 # Kevin doesn't use
    reward_compile_weight: 0.0 # Kevin doesn't use
    reward_correctness_weight: 0.3 # Kevin uses 0.3 for binary correctness
    reward_speed_weight: 1.0 # Kevin adds speedup directly
    reward_length_weight: 0.0 # Kevin: causes response collapse
    reward_thinking_weight: 0.0 # Kevin removes thinking from prompts

    # Test split
    test_fraction: 0.1

    # Renderer (should match model)
    renderer_name: "qwen3"

    # RA-ICL Configuration (retrieval-augmented prompts)
    prompt_option: "raicl"
    rag_index_path: "./kernel_rag_index"
    raicl_k: 3 # Number of examples to retrieve

    # ==========================================================================
    # Modal Configuration (Isolated GPU Evaluation)
    # Run each kernel in an isolated Modal container with hard timeout
    # ==========================================================================
    use_modal: true # Enable Modal for isolated kernel evaluation
    modal_gpu_type: "A100" # GPU type: A100, H100, L40S, T4
    modal_timeout: 60.0 # Hard timeout per kernel (seconds)
